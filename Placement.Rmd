---
title: "Placement"
author: "Yicheng Shen"
date: "Aug 6, 2024"
output: 
    pdf_document    
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, eval = T, cache = F, warning = F, message = F)
library(tidyverse)
library(rvest)
library(ggplot2)
library(gridExtra)
ggplot2::theme_set(ggplot2::theme_bw())
knitr::opts_chunk$set(out.width = "100%", fig.align = 'center')
```



Note: the github repo for this work is: https://github.com/sheny2/Placement

**Q1**

*Task 1: Read the data*

Scrape the wikipedia page on natural disasters
```{r}
url <- "https://en.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll"

webpage <- read_html(url)
tables <- webpage %>% html_nodes("table.wikitable")

# quick check: 20th and 21st century data are in the 2nd and 3rd table of this page
table_20th <- tables[[2]] %>% html_table(fill = TRUE)
table_21st <- tables[[3]] %>% html_table(fill = TRUE)

disasters <- rbind(table_20th, table_21st) %>% as_tibble() # merge into one
head(disasters)
```


*Task 2: Clean the data*

Convert the death toll to numbers using the midpoints when a range is given and the bound when an upper or lower bound is given.

```{r}
disasters_clean = disasters
# remove unnecessary footnote, words and symbols
disasters_clean$`Death toll` = disasters$`Death toll` %>%
                                            str_remove_all(",") %>%
                                            str_remove_all("\\[.*?\\]") %>%  
                                            str_remove_all("[a-zA-Z]") %>%   
                                            str_remove_all("\\(.*?\\)") %>%  
                                            str_trim()

# which entries need special attention to clean
index_to_clean = is.na(as.numeric(gsub("," ,"", disasters_clean$`Death toll`)))
disasters_clean$`Death toll`[index_to_clean]

# use regular expression here (there seems to be two kinds of dashes here)
convert_death_toll <- function(toll) {
  if (str_detect(toll, "\\d+\\s*â€“\\s*\\d+")) {
    nums <- str_extract_all(toll, "\\d+")[[1]] %>% as.numeric()
    return(mean(nums))
  } else if (str_detect(toll, "\\d+\\s*-\\s*\\d+")) {
    nums <- str_extract_all(toll, "\\d+")[[1]] %>% as.numeric()
    return(mean(nums))
  } else if (str_detect(toll, "\\d+")) {
    return(as.numeric(str_extract(toll, "\\d+")))
  } 
}

death_toll_clean = round(sapply(disasters_clean$`Death toll`, convert_death_toll)) 
death_toll_clean[index_to_clean] # check the results of those irregular ones

disasters_clean$`Death toll` = death_toll_clean
disasters_clean
```


*Task 3: Plot the data*

Bar plots seem to be a better way of visualization here.
Deadly disasters have happened frequently throughout the years. 
While at one time floods caused one of the highest death tolls, it should be noted that earthquakes appear more as the leading ones causing fatalities. Both topical cyclones and earthquakes seem to be the very frequent and deadly disasters during the studied period.

```{r}
table(disasters_clean$Type)
disasters_clean$Type = ifelse(disasters_clean$Type == "Heat Wave", "Heat wave", disasters_clean$Type)
disasters_clean %>% group_by(Type) %>% summarise(Count = n(), `Total Death Toll` = sum(`Death toll`), `Average` = mean(`Death toll`), `Median` = median(`Death toll`))
```

```{r, fig.width=10, fig.height=4, eval = F, echo = F}
ggplot(disasters_clean, aes(x = Year, y = `Death toll`, color = Type)) +
  geom_point() +
  labs(title = "Death Toll of Natural Disasters by Year",
       x = "Year", y = "Death Toll", color = "Type of Disaster")
```



```{r, fig.width=12, fig.height=5}
p1 = ggplot(disasters_clean, aes(x = Year, y = `Death toll`, color = Type)) +
  geom_point() +
  labs(title = "Death Toll of Natural Disasters by Year",
       x = "Year", y = "Death Toll", color = "Type of Disaster")
p2 = ggplot(disasters_clean %>% filter(Event != "1931 China floods"), 
       aes(x = Year, y = `Death toll`, color = Type)) +
  geom_point() +
  labs(title = "Death Toll of Natural Disasters by Year without one outlier",
       x = "Year", y = "Death Toll", color = "Type of Disaster")

grid.arrange(arrangeGrob(p1 + theme(legend.position="none"), 
                         p2 + theme(legend.position="none"), 
                         nrow = 1),
             lemon::g_legend(p1 +  guides(colour = guide_legend(nrow = 2))), 
             nrow = 2, heights = c(10, 3))
```

```{r, fig.width=12, fig.height=5}
p1 = ggplot(disasters_clean, aes(x = Year, y = `Death toll`, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Death Toll of Natural Disasters by Year",
       x = "Year", y = "Death Toll", fill = "Type of Disaster")
p2 = ggplot(disasters_clean %>% filter(Event != "1931 China floods"),
       aes(x = Year, y = `Death toll`, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Death Toll of Natural Disasters by Year without one outlier",
       x = "Year", y = "Death Toll", fill = "Type of Disaster")

grid.arrange(arrangeGrob(p1 + theme(legend.position="none"), 
                         p2 + theme(legend.position="none"), 
                         nrow = 1),
             lemon::g_legend(p1 +  guides(fill = guide_legend(nrow = 2))), 
             nrow = 2, heights = c(10, 3))
```


**Q2**

First we can derive the form of gradient by computing the derivative w.r.t b:
$$\frac{\partial ||y-bx||^2}{\partial b} = 2(\frac{\partial \sum_i^n (y_i-bx_i)}{\partial b}) = 2(\frac{- \sum_i^n x_i (y_i-bx_i)}{n})$$

```{r}
# write the gradient descent function
gradient_descent <- function(x, y, learning_rate, num_iter) {
  b <- 0 # start at 0, or somewhere
  n <- length(x)
  
  for (i in 1:num_iter) {
    gradient <- -2 * sum(x * (y - b * x)) / n # compute gradient
    b <- b - learning_rate * gradient # update here
  }
  return(b)
}

# Test the function using randomly generated normal vectors
set.seed(8848) 
n <- 100
x <- rnorm(n)
b <- 5
y <- b * x + rnorm(n)

S <- 1000 # long enough

b_grad <- gradient_descent(x, y, learning_rate = 0.05, num_iter = S) # choose e to be 0.05

# Compare
b_solution <- x %*% y / (norm(x, type="2"))^2
b_solution %>% as.vector()
b_grad # success! 
```

```{r, out.width="80%"}
# To test different learning rates
test_learning_rates <- function(x, y, b, learning_rates, num_iter) {
  results <- data.frame(learning_rate = c(), b_est = c())
  
  for (e in learning_rates) {
    b_est <- gradient_descent(x, y, e, num_iter)
    results <- rbind(results, data.frame(learning_rate = e, b_est = b_est))
  }
  
  return(results)
}
```

**Findings**: The algorithm can perform well usually when we choose a reasonable value for the learning rate, but it does not guarantee to work at all situations.

We can see from below that the learning rate should not be too small. A too-small step size can make it inefficient for the algorithm to explore and reach convergence or get stuck at a local min instead of global one, thus unable to find the correct solution. 

Meanwhile, we also do not want the step size to be too large at each update, which could cause our estimate to oscillate too much and miss the optimal point as well. 


```{r, out.width="70%"}
learning_rates <- seq(0.001, 0.1, by = 0.001)
results <- test_learning_rates(x, y, b_true, learning_rates, num_iter = S)

# Plot the results
ggplot(results, aes(x = learning_rate, y = b_est)) +
  geom_line() +
  labs(title = "Estimate versus Learning Rate", x = "Learning Rate", y = "Estimate")
```










